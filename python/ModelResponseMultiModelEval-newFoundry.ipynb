{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7941148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "azure_ai_project = os.environ.get(\"AZURE_PROJECT_ENDPOINT\")\n",
    "azure_openai_deployment = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_openai_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")  \n",
    "azure_openai_api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "# Function to create model-specific configuration\n",
    "def get_model_config(model_name):\n",
    "\n",
    "    api_key = azure_openai_api_key\n",
    "    \n",
    "    if model_name == \"grok\":\n",
    "        return {\n",
    "            \"azure_endpoint\": azure_openai_endpoint,\n",
    "            \"azure_deployment\": \"grok-4\",\n",
    "            \"api_key\": api_key,\n",
    "            \"api_version\": \"2024-05-01-preview\",\n",
    "        }\n",
    "    elif model_name == \"gpt5\":\n",
    "        return {\n",
    "            \"azure_endpoint\": azure_openai_endpoint,\n",
    "            \"azure_deployment\": \"gpt-5-pro\",\n",
    "            \"api_key\": api_key,\n",
    "            \"api_version\": \"2024-12-01-preview\",\n",
    "        }\n",
    "    elif model_name == \"claude\":\n",
    "        return {\n",
    "            \"azure_endpoint\": azure_openai_endpoint,\n",
    "            \"azure_deployment\": \"claude-sonnet-4-5\",\n",
    "            \"api_key\": api_key,\n",
    "            \"api_version\": \"20250929\",\n",
    "        }\n",
    "    else:\n",
    "        # Default configuration from environment variables\n",
    "        api_version = azure_openai_api_version\n",
    "        if api_version and api_version < \"2024-12-01-preview\":\n",
    "            api_version = \"2024-12-01-preview\"\n",
    "        \n",
    "        return {\n",
    "            \"azure_endpoint\": azure_openai_endpoint,\n",
    "            \"azure_deployment\": azure_openai_deployment,\n",
    "            \"api_key\": azure_openai_api_key,\n",
    "            \"api_version\": api_version,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c497a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Credential imports\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "!az login\n",
    "\n",
    "# Initialize Azure credentials\n",
    "credential = AzureCliCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ed125",
   "metadata": {},
   "source": [
    "Get all ground trut to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf40fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data/software_engineering_data.jsonl\"\n",
    "\n",
    "# Load the dataset content from the local file\n",
    "with open(path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "ground_truth = [item[\"ground_truth\"] for item in data]\n",
    "print(f\"Loaded {len(ground_truth)} ground truth values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a8368",
   "metadata": {},
   "source": [
    "Setup AI Foundry projec client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient \n",
    "\n",
    "# Create the project client (Foundry project and credentials): \n",
    "\n",
    "project_client = AIProjectClient( \n",
    "    endpoint=azure_ai_project, \n",
    "    credential=credential, \n",
    ")\n",
    "print(\"Creating an OpenAI client from the AI Project client\")\n",
    "client = project_client.get_openai_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from openai.types.evals.create_eval_jsonl_run_data_source_param import (\n",
    "    CreateEvalJSONLRunDataSourceParam,\n",
    "    SourceFileID,\n",
    ")\n",
    "from azure.ai.projects.models import (\n",
    "    DatasetVersion,\n",
    ")\n",
    "\n",
    "def get_responses(path, azure_openai_deployment):\n",
    "    #dataset: DatasetVersion = project_client.datasets.upload_file(\n",
    "    #    name=\"software_engineering_data\",\n",
    "    #    version=1,\n",
    "    #    file_path=path,\n",
    "    #)\n",
    "    #print(dataset)\n",
    "    #dataset: DatasetVersion = project_client.datasets.get(\n",
    "    #    name=\"software_engineering_data\",\n",
    "    #    version=1,\n",
    "    #)\n",
    "\n",
    "    # Load the dataset content from the local file\n",
    "    with open(path, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    conversation = client.conversations.create(\n",
    "            items=[{\"type\": \"message\", \"role\": \"user\", \"content\": item[\"query\"]} for item in data],\n",
    "    )\n",
    "    print(f\"Created conversation with {len(data)} user messages (id: {conversation.id})\")\n",
    "\n",
    "    response = client.responses.create(\n",
    "        conversation=conversation.id,\n",
    "        model=azure_openai_deployment,\n",
    "        input=\"\",\n",
    "    )\n",
    "    print(f\"Response output: {response.output_text} (id: {response.id})\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ae789",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_config = {\"type\": \"azure_ai_source\", \"scenario\": \"responses\"}\n",
    "\n",
    "evaluation_criteria = [\n",
    "            {\n",
    "                \"type\": \"azure_ai_evaluator\", \n",
    "                \"name\": \"violence_detection\", \n",
    "                \"evaluator_name\": \"builtin.violence\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"azure_ai_evaluator\",\n",
    "                \"name\": \"self_harm\",\n",
    "                \"evaluator_name\": \"builtin.self_harm\",\n",
    "                \"evaluator_version\": \"1\",\n",
    "            },\n",
    "            {\"type\": \"azure_ai_evaluator\", \"name\": \"Sexual\", \"evaluator_name\": \"builtin.sexual\", \"evaluator_version\": \"1\"},\n",
    "            {\n",
    "                \"type\": \"azure_ai_evaluator\",\n",
    "                \"name\": \"Hate Unfairness\",\n",
    "                \"evaluator_name\": \"builtin.hate_unfairness\",\n",
    "                \"evaluator_version\": \"1\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"azure_ai_evaluator\",\n",
    "                \"name\": \"coherence\",\n",
    "                \"evaluator_name\": \"builtin.coherence\",\n",
    "                \"initialization_parameters\": {\n",
    "                    \"deployment_name\": f\"{azure_openai_deployment}\"\n",
    "                },\n",
    "            },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d38169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "eval_object = client.evals.create(\n",
    "        name=\"vk-2255-eval_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "        data_source_config=data_source_config,\n",
    "        testing_criteria=evaluation_criteria,\n",
    ")\n",
    "\n",
    "def evaluate_model(response, model_name):\n",
    "    data_source = {\n",
    "            \"type\": \"azure_ai_responses\",\n",
    "            \"item_generation_params\": {\n",
    "                \"type\": \"response_retrieval\",\n",
    "                \"data_mapping\": {\"response_id\": \"{{item.resp_id}}\"},\n",
    "                \"source\": {\"type\": \"file_content\", \"content\": [{\"item\": {\"resp_id\": response.id}}]},\n",
    "            },\n",
    "    }\n",
    "    \n",
    "    results = client.evals.runs.create(\n",
    "        eval_id=eval_object.id,\n",
    "        name=\"eval_id_run_\" + model_name + \"_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "        data_source=data_source,\n",
    "    )\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fda45",
   "metadata": {},
   "source": [
    "Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682dface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"grok\",\n",
    "    \"gpt5\",\n",
    "    \"claude\",\n",
    "]\n",
    "\n",
    "# Define a simple answer length evaluator\n",
    "def answer_length(response, **kwargs):\n",
    "    return {\"answer_length\": len(response)}\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data/software_engineering_data.jsonl\"\n",
    "\n",
    "for model in models:\n",
    "    # Get model-specific configuration\n",
    "    model_config = get_model_config(model)\n",
    "    \n",
    "    print(f\"Running evaluation for model: {model}\")\n",
    "    response = get_responses(path, model_config[\"azure_deployment\"])\n",
    "    evaluate_model(response, model)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
